{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Different Class Selections and Different Number of Descendants: Recongition Gap, MIRC-percentage and MIRC sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates the bar plot figures of the magnitudes and\n",
    "standard deviations of recognition gaps, the sizes of the MIRCs and the\n",
    "fractions of images that have a MIRC for different experiments. In the\n",
    "manuscript, it is Figure 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please specify the path to the most top directory of your recognition\n",
    "gap experiments, i.e. the parent directory of the analysis folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_csv_utils\n",
    "import plot_utils\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "path_to_recognition_gap_folder = \"/gpfs01/bethge/home/jborowski/CHAM_recognition_gap/JOV_publication_git_bethgelab/recognition_gap/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# custom imports\n",
    "sys.path.insert(1, path_to_recognition_gap_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dictionaries that will be plotted\n",
    "recognitionGapsMachine = {}\n",
    "recognitionGapsMachineStd = {}\n",
    "number_of_MIRCS = {}\n",
    "total_number_individual_image_classes = {}\n",
    "MIRC_size_mean = {}\n",
    "MIRC_size_std = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over each experiment with different conditions\n",
    "for exp_dir_condition, exp_dir_list in data_csv_utils.exp_dir_dict.items():\n",
    "    # get all the data in one dataframe\n",
    "    all_data_df = data_csv_utils.get_df_from_exp_dir_list(exp_dir_list)\n",
    "    # clean the data such that only data from images which yielded MIRCs is\n",
    "    # contained\n",
    "    all_data_df_real_MIRCs = data_csv_utils.get_df_with_data_from_real_MIRCs_only(\n",
    "        all_data_df)\n",
    "\n",
    "    # calculate the metrics that are displayed in the appendix:\n",
    "    # A. mean and standard deviation of rec_gap\n",
    "    recognitionGapsMachine[exp_dir_condition] = all_data_df_real_MIRCs.mean(\n",
    "        axis=0).rec_gap\n",
    "    recognitionGapsMachineStd[exp_dir_condition] = all_data_df_real_MIRCs.std(\n",
    "        axis=0, ddof=0).rec_gap\n",
    "\n",
    "    # B. mean and std of MIRCs size\n",
    "    MIRC_size_mean[exp_dir_condition] = all_data_df_real_MIRCs.mean(\n",
    "        axis=0).pix_size_MIRC\n",
    "    MIRC_size_std[exp_dir_condition] = all_data_df_real_MIRCs.std(\n",
    "        axis=0, ddof=0).pix_size_MIRC\n",
    "\n",
    "    # C. calculate number of total images and number of images with MIRCs.\n",
    "    total_number_individual_image_classes[exp_dir_condition] = all_data_df.shape[0]\n",
    "    number_of_MIRCS[exp_dir_condition] = all_data_df_real_MIRCs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the data from Ullman et al. (2016) to the dictionary\n",
    "recognitionGapsMachine[\"human-selected patches\"] = 0.14\n",
    "recognitionGapsMachineStd[\"human-selected patches\"] = 0.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiment_conditions = len(data_csv_utils.exp_dir_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Recognition gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_machine_rec_gaps_as_vertical_bars():\n",
    "    \"\"\"plot the magnitued and standard deviation of the recognition gaps for the different\n",
    "    machine experiments as vertical bars.\"\"\"\n",
    "\n",
    "    for x_index, (key, value) in enumerate(recognitionGapsMachine.items()):\n",
    "        edgecolor = \"k\" if key == \"human-selected patches\" else \"\"\n",
    "        plt.bar(\n",
    "            x_index,\n",
    "            value,\n",
    "            plot_utils.width,\n",
    "            yerr=recognitionGapsMachineStd[key],\n",
    "            color=plot_utils.colors[x_index],\n",
    "            edgecolor=edgecolor,\n",
    "            label=key,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[4.5, 4.5])\n",
    "\n",
    "# plot data\n",
    "# plot human data as horizontal bar\n",
    "plot_utils.plot_human_rec_gap_as_horizonal_bar(len(recognitionGapsMachine))\n",
    "# plot machine data as vertical bar\n",
    "plot_machine_rec_gaps_as_vertical_bars()\n",
    "\n",
    "# axes\n",
    "ax = plt.axes()\n",
    "x_labels = [\n",
    "    f\"{round(recognitionGapsMachine[key], 3)}\"\n",
    "    f\"\\u00B1\"  # plus-minus sign\n",
    "    f\"{round(recognitionGapsMachineStd[key], 3)}\"\n",
    "    for key in recognitionGapsMachine.keys()]\n",
    "plt.xticks(list(range(n_experiment_conditions + 1)), x_labels, rotation=20)\n",
    "ax.set_xlim(-0.5, n_experiment_conditions + 0.5)\n",
    "ax.set_ylabel(\"recognition gap\")\n",
    "ax.yaxis.label.set_color(\"red\")\n",
    "\n",
    "# legend\n",
    "legend = plt.legend(bbox_to_anchor=(1.3, 0.6765), frameon=False)\n",
    "\n",
    "plot_utils.hide_right_and_top_spine(plt.axes())\n",
    "\n",
    "plt.title(\"Recognition gap: mean and std\")\n",
    "\n",
    "plt.savefig(\"JOV_appendix_rec_gap.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Size of MIRCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[4, 4.5])\n",
    "\n",
    "# plot size and standard deviation of MIRCs as vertical bars\n",
    "for x_index, (key, value) in enumerate(MIRC_size_mean.items()):\n",
    "    p = plt.bar(\n",
    "        x_index,\n",
    "        value,\n",
    "        plot_utils.width,\n",
    "        yerr=MIRC_size_std[key],\n",
    "        color=plot_utils.colors[x_index],\n",
    "    )\n",
    "\n",
    "# axes\n",
    "x_labels = [\n",
    "    f\"{round(MIRC_size_mean[key], 3)}\"\n",
    "    f\"\\u00B1\"  # plus-minus sign\n",
    "    f\"{round(MIRC_size_std[key], 3)}\"\n",
    "    for key in MIRC_size_mean.keys()]\n",
    "plt.xticks(list(range(n_experiment_conditions)), x_labels, rotation=20)\n",
    "plt.xlim(-0.5, n_experiment_conditions - 1 + 0.5)\n",
    "plt.ylabel(\"mean size of MIRCs [original px space]\")\n",
    "\n",
    "plot_utils.hide_right_and_top_spine(plt.axes())\n",
    "\n",
    "plt.title(\"Size of MIRCs: mean and std\")\n",
    "\n",
    "plt.savefig(\"JOV_appendix_MIRC_size.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. MIRC%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[4, 4.5])\n",
    "\n",
    "# plot fraction of MIRCs as vertical bars\n",
    "for x_index, (key, value) in enumerate(number_of_MIRCS.items()):\n",
    "    p = plt.bar(\n",
    "        x_index,\n",
    "        number_of_MIRCS[key] / total_number_individual_image_classes[key],\n",
    "        plot_utils.width,\n",
    "        color=plot_utils.colors[x_index]\n",
    "    )\n",
    "\n",
    "# axes\n",
    "x_labels = [\n",
    "    f\"{number_of_MIRCS[key]}/{total_number_individual_image_classes[key]}\"\n",
    "    for key in number_of_MIRCS.keys()]\n",
    "plt.xticks(list(range(n_experiment_conditions)), x_labels, rotation=20)\n",
    "plt.xlim(-0.5, n_experiment_conditions - 1 + 0.5)\n",
    "plt.xlabel(\"number of images with MIRCs / total number of images\")\n",
    "ylabel = plt.ylabel(\"fraction of images that has MIRCs\")\n",
    "plt.ylim(0.0, 1.05)\n",
    "\n",
    "plot_utils.hide_right_and_top_spine(plt.axes())\n",
    "\n",
    "plt.title(\"Fraction of images that has MIRCs\")\n",
    "\n",
    "plt.savefig(\"JOV_appendix_MIRC_fraction.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
